{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d0d2ab-838c-4816-80e9-07a0ab41bf07",
   "metadata": {},
   "source": [
    "# üìä Preparing Tableau Dashboards for Credit Risk Insights\n",
    "\n",
    "In this final notebook, I prepare targeted CSV files from our validation dataset and SHAP outputs to support **interactive Tableau dashboards**. These dashboards are designed to communicate key insights from the credit risk modeling pipeline to both technical and non-technical audiences.\n",
    "\n",
    "### üéØ Objectives\n",
    "\n",
    "- Export curated datasets to power five core Tableau visualizations:\n",
    "  1. SHAP summary statistics for global feature importance\n",
    "  2. Risk distribution by predicted probability\n",
    "  3. Feature importance by applicant group (e.g., default vs non-default)\n",
    "  4. Confusion matrix with precision/recall metrics\n",
    "  5. Individual applicant-level SHAP values for drill-down explanations\n",
    "\n",
    "Each export is tailored to maximize clarity, interactivity, and storytelling impact inside Tableau Public.\n",
    "\n",
    "> This notebook acts as the **bridge between machine learning outputs and stakeholder communication**, enabling the delivery of interpretable, transparent credit risk insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc152078-b47f-4296-a015-84739827df46",
   "metadata": {},
   "source": [
    "### üì¶ Load Required Libraries\n",
    "\n",
    "I begin by importing the essential libraries for this final notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766d9fb6-c901-4666-aaa1-2008ec912ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model loading and evaluation tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# SHAP explanations and LightGBM model compatibility\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Suppress SHAP warning about binary classifiers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322f8ce-1b42-43c7-84ae-10654a8ae804",
   "metadata": {},
   "source": [
    "### üß† Load Trained Model and Validation Data\n",
    "\n",
    "I begin by loading the final trained LightGBM model along with the validation dataset and corresponding labels. These will be used to generate prediction outputs and SHAP values for Tableau-ready visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7261ade6-0e85-4703-a095-8a4da11aeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final trained LightGBM model\n",
    "model = joblib.load(\"../models/lgbm_model.joblib\")\n",
    "\n",
    "# Load the validation feature set\n",
    "X_valid = pd.read_parquet(\"../data/processed/X_valid.parquet\")\n",
    "\n",
    "# Load the corresponding validation labels\n",
    "y_valid = pd.read_parquet(\"../data/processed/y_valid.parquet\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f52869-2bee-48e7-8303-4eb0ce39d5e2",
   "metadata": {},
   "source": [
    "### üìä Prepare Global SHAP Summary for Tableau Dashboarding\n",
    "\n",
    "In this section, I generate a summary of global feature importance using SHAP values and prepare it for visualization in Tableau.\n",
    "\n",
    "- First, I predict loan default probabilities on the validation set and assign class labels based on the chosen threshold (0.3).\n",
    "- I then compute **SHAP values** using `TreeExplainer`, which is optimized for our LightGBM model.\n",
    "- To summarize global importance, we calculate the **mean absolute SHAP value per feature**, giving us a ranked list of the most influential variables.\n",
    "- Finally, I save this summary to a CSV file (`global_shap_importance.csv`) in the `data/final/` directory. This file can be easily loaded into Tableau to build an interactive feature importance dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8e91ce-021d-4444-8668-0cc6e94db619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Global SHAP feature importance saved to global_shap_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and assign predicted labels using chosen threshold\n",
    "y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "y_pred_thresh = (y_pred_proba >= 0.3).astype(int)\n",
    "\n",
    "# Add prediction columns to X_valid\n",
    "X_valid_final = X_valid.copy()\n",
    "X_valid_final[\"loan_default_proba\"] = y_pred_proba\n",
    "X_valid_final[\"predicted_label\"] = y_pred_thresh\n",
    "X_valid_final[\"actual_label\"] = y_valid.values\n",
    "\n",
    "# Initialize SHAP explainer (TreeExplainer for LightGBM)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values (shape: [n_samples, n_features])\n",
    "shap_values = explainer.shap_values(X_valid)\n",
    "\n",
    "# Compute global SHAP feature importance (mean absolute value)\n",
    "global_importance = (\n",
    "    pd.DataFrame(shap_values, columns=X_valid.columns)\n",
    "    .abs()\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"feature_name\", 0: \"mean_abs_shap_value\"})\n",
    "    .sort_values(\"mean_abs_shap_value\", ascending=False)\n",
    ")\n",
    "\n",
    "# Save global SHAP importance summary for Tableau\n",
    "global_importance.to_csv(\"../data/final/global_shap_importance.csv\", index=False)\n",
    "print(\"‚úÖ Global SHAP feature importance saved to global_shap_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e75ed-a5d2-4815-8939-79ec4daf7ad7",
   "metadata": {},
   "source": [
    "### üìä Save Risk Distribution Data for Tableau\n",
    "\n",
    "To support visual analysis of loan default risk scores in Tableau, I prepare a dataset containing key applicant features and the model‚Äôs predicted probabilities:\n",
    "\n",
    "- `loan_default_proba`: Model‚Äôs predicted probability of default\n",
    "- `actual_label`: Ground truth indicating default or not\n",
    "- `DAYS_BIRTH`, `AMT_INCOME_TOTAL`, `DAYS_EMPLOYED`: Key demographic and financial indicators\n",
    "- `label`: Readable label version of the actual class (e.g., ‚ÄúDefault‚Äù, ‚ÄúNo Default‚Äù)\n",
    "\n",
    "This file can be used to create histograms, density plots, or stratified risk profiles in Tableau dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15b70d1-b765-4197-b677-f2f2b7c386bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Risk distribution saved to risk_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# Add readable labels to indicate default vs no default\n",
    "X_valid_final[\"label\"] = X_valid_final[\"actual_label\"].map({0: \"No Default\", 1: \"Default\"})\n",
    "\n",
    "# Select relevant columns for visualization\n",
    "risk_df = X_valid_final[[\n",
    "    \"loan_default_proba\",\n",
    "    \"actual_label\",\n",
    "    \"DAYS_BIRTH\",\n",
    "    \"AMT_INCOME_TOTAL\",\n",
    "    \"DAYS_EMPLOYED\",\n",
    "    \"label\"\n",
    "]]\n",
    "\n",
    "# Save to CSV\n",
    "risk_df.to_csv(\"../data/final/risk_distribution.csv\", index=False)\n",
    "print(\"‚úÖ Risk distribution saved to risk_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67e64e1-1dab-4f5b-a798-a8e0bb2b2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP aggregation saved for Tableau!\n"
     ]
    }
   ],
   "source": [
    "shap_df = pd.read_csv(\"../data/final/shap_values_tableau.csv\")\n",
    "\n",
    "# üè∑Ô∏è Create risk bands from predicted probabilities\n",
    "bins = [0, 0.2, 0.5, 1.0]\n",
    "labels = [\"Low Risk\", \"Medium Risk\", \"High Risk\"]\n",
    "shap_df[\"risk_band\"] = pd.cut(shap_df[\"loan_default_proba\"], bins=bins, labels=labels)\n",
    "\n",
    "# üîù Top 15 features based on mean absolute SHAP value\n",
    "mean_abs_shap = shap_df.drop(columns=[\"loan_default_proba\", \"predicted_label\", \"risk_band\"]).abs().mean()\n",
    "top_features = mean_abs_shap.sort_values(ascending=False).head(15).index.tolist()\n",
    "\n",
    "# üìä Melt and group for Tableau\n",
    "shap_melted = shap_df[[\"risk_band\"] + top_features].melt(id_vars=\"risk_band\", var_name=\"feature\", value_name=\"shap_value\")\n",
    "agg_df = shap_melted.groupby([\"risk_band\", \"feature\"], observed=True).mean().reset_index()\n",
    "\n",
    "# üíæ Save for Tableau\n",
    "agg_df.to_csv(\"../data/final/agg_shap_by_risk_band.csv\", index=False)\n",
    "print(\"‚úÖ SHAP aggregation saved for Tableau!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f79f113-58b1-47ca-a3e5-c9ddd914b3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: shap_vs_risk_long.csv\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Prepare SHAP vs Risk Score visualization data in long format for Tableau\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the saved shap_vs_risk.csv\n",
    "df = pd.read_csv(\"../data/final/shap_vs_risk.csv\")\n",
    "\n",
    "# 2. Define correct column names\n",
    "feature_cols = [\n",
    "    \"EXT_SOURCE_1\",\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"EXT_SOURCE_3\",\n",
    "    \"credit_annuity_ratio\",\n",
    "    \"credit_goods_ratio\",\n",
    "    \"CODE_GENDER_M\",\n",
    "    \"DAYS_BIRTH\",\n",
    "    \"ORGANIZATION_TYPE_TE\"\n",
    "]\n",
    "\n",
    "# 3. Melt into long format\n",
    "long_df = df.melt(\n",
    "    id_vars=[\"loan_default_proba\"],\n",
    "    value_vars=feature_cols,\n",
    "    var_name=\"Feature\",\n",
    "    value_name=\"SHAP Value\"\n",
    ")\n",
    "\n",
    "# 4. Save to CSV for Tableau\n",
    "long_df.to_csv(\"../data/final/shap_vs_risk_long.csv\", index=False)\n",
    "print(\"‚úÖ Saved: shap_vs_risk_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e5ac9d-2630-416e-9cf4-fffc6ac4fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP long-format file saved!\n"
     ]
    }
   ],
   "source": [
    "# üîÑ Melt SHAP values from wide to long format\n",
    "shap_long = shap_df.melt(\n",
    "    id_vars=[\"loan_default_proba\", \"predicted_label\", \"actual_label\"],\n",
    "    var_name=\"feature\",\n",
    "    value_name=\"shap_value\"\n",
    ")\n",
    "\n",
    "# üíæ Save for Tableau\n",
    "shap_long.to_csv(\"../data/final/shap_values_long.csv\", index=False)\n",
    "print(\"‚úÖ SHAP long-format file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96467c5-def4-4988-bdb4-9a496b8f23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: confusion_matrix_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports (in case not already)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ‚úÖ Calculate confusion matrix components\n",
    "cm = confusion_matrix(X_valid_final[\"actual_label\"], X_valid_final[\"predicted_label\"])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# üßæ Create a summary DataFrame\n",
    "confusion_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"True Negative\", \"False Positive\", \"False Negative\", \"True Positive\"],\n",
    "    \"Count\": [tn, fp, fn, tp]\n",
    "})\n",
    "\n",
    "# üíæ Save the confusion matrix summary\n",
    "confusion_summary.to_csv(\"../data/final/confusion_matrix_summary.csv\", index=False)\n",
    "print(\"‚úÖ Saved: confusion_matrix_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07963872-0c6e-4a97-a8c3-a4b585ada2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: confusion_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# üßÆ Save row-level predictions\n",
    "X_valid_final[[\"loan_default_proba\", \"predicted_label\", \"actual_label\"]].to_csv(\n",
    "    \"../data/final/confusion_predictions.csv\", index=False\n",
    ")\n",
    "print(\"‚úÖ Saved: confusion_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618c73dc-c638-4c28-946f-838a30d1626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined file saved with consistent dtypes and no warnings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your two CSVs\n",
    "summary_df = pd.read_csv(\"../data/final/confusion_matrix_summary.csv\")\n",
    "pred_df = pd.read_csv(\"../data/final/confusion_predictions.csv\")\n",
    "\n",
    "# Add missing columns with explicit types\n",
    "summary_df[\"loan_default_proba\"] = pd.Series([np.nan] * len(summary_df), dtype=\"float64\")\n",
    "summary_df[\"actual_label\"] = pd.Series([np.nan] * len(summary_df), dtype=\"float64\")\n",
    "summary_df[\"predicted_label\"] = pd.Series([np.nan] * len(summary_df), dtype=\"float64\")\n",
    "summary_df[\"Source\"] = \"Summary\"\n",
    "\n",
    "pred_df[\"Metric\"] = pd.Series([pd.NA] * len(pred_df), dtype=\"string\")\n",
    "pred_df[\"Count\"] = pd.Series([np.nan] * len(pred_df), dtype=\"float64\")\n",
    "pred_df[\"Source\"] = \"Prediction\"\n",
    "\n",
    "# Define consistent column order\n",
    "combined_cols = [\"Metric\", \"Count\", \"loan_default_proba\", \"actual_label\", \"predicted_label\", \"Source\"]\n",
    "\n",
    "# Reorder and ensure alignment\n",
    "summary_part = summary_df[combined_cols]\n",
    "pred_part = pred_df[combined_cols]\n",
    "\n",
    "# Concatenate without warning\n",
    "combined_df = pd.concat([summary_part, pred_part], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "combined_df.to_csv(\"../data/final/confusion_combined.csv\", index=False)\n",
    "print(\"‚úÖ Combined file saved with consistent dtypes and no warnings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d542704-7186-4e7b-a7fa-610ca48c74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Shape: (61507, 6)\n",
      "üßæ Column names: ['Metric', 'Count', 'loan_default_proba', 'actual_label', 'predicted_label', 'Source']\n",
      "üìå Sample rows:\n",
      "           Metric    Count  loan_default_proba  actual_label  predicted_label  \\\n",
      "0   True Negative  55095.0                 NaN           NaN              NaN   \n",
      "1  False Positive   1443.0                 NaN           NaN              NaN   \n",
      "2  False Negative   4122.0                 NaN           NaN              NaN   \n",
      "3   True Positive    843.0                 NaN           NaN              NaN   \n",
      "4             NaN      NaN            0.032648           0.0              0.0   \n",
      "5             NaN      NaN            0.060563           0.0              0.0   \n",
      "6             NaN      NaN            0.282761           0.0              0.0   \n",
      "7             NaN      NaN            0.076722           0.0              0.0   \n",
      "8             NaN      NaN            0.081543           0.0              0.0   \n",
      "9             NaN      NaN            0.078865           0.0              0.0   \n",
      "\n",
      "       Source  \n",
      "0     Summary  \n",
      "1     Summary  \n",
      "2     Summary  \n",
      "3     Summary  \n",
      "4  Prediction  \n",
      "5  Prediction  \n",
      "6  Prediction  \n",
      "7  Prediction  \n",
      "8  Prediction  \n",
      "9  Prediction  \n",
      "üéØ Unique values in Actual and Predicted Labels:\n",
      "  - Actual: [nan  0.  1.]\n",
      "  - Predicted: [nan  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/final/confusion_combined.csv\")\n",
    "\n",
    "print(\"üî¢ Shape:\", df.shape)\n",
    "print(\"üßæ Column names:\", df.columns.tolist())\n",
    "print(\"üìå Sample rows:\")\n",
    "print(df.head(10))\n",
    "print(\"üéØ Unique values in Actual and Predicted Labels:\")\n",
    "print(\"  - Actual:\", df[\"actual_label\"].unique())\n",
    "print(\"  - Predicted:\", df[\"predicted_label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7ae8c2-d084-4f80-99c4-bcc469fed35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned file saved!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/final/confusion_combined.csv\")\n",
    "\n",
    "# ‚úÖ Keep only the prediction rows\n",
    "df_pred = df[df[\"Source\"] == \"Prediction\"].copy()\n",
    "\n",
    "# ‚úÖ Save clean version for Tableau\n",
    "df_pred.to_csv(\"../data/final/confusion_prediction_only.csv\", index=False)\n",
    "print(\"‚úÖ Cleaned file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95398efa-2f59-4ef8-864b-e451a671effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Shape: (61503, 6)\n",
      "üßæ Column names: ['Metric', 'Count', 'loan_default_proba', 'actual_label', 'predicted_label', 'Source']\n",
      "üìå Sample rows:\n",
      "   Metric  Count  loan_default_proba  actual_label  predicted_label  \\\n",
      "0     NaN    NaN            0.032648           0.0              0.0   \n",
      "1     NaN    NaN            0.060563           0.0              0.0   \n",
      "2     NaN    NaN            0.282761           0.0              0.0   \n",
      "3     NaN    NaN            0.076722           0.0              0.0   \n",
      "4     NaN    NaN            0.081543           0.0              0.0   \n",
      "5     NaN    NaN            0.078865           0.0              0.0   \n",
      "6     NaN    NaN            0.004224           0.0              0.0   \n",
      "7     NaN    NaN            0.004712           0.0              0.0   \n",
      "8     NaN    NaN            0.402899           0.0              1.0   \n",
      "9     NaN    NaN            0.087414           1.0              0.0   \n",
      "\n",
      "       Source  \n",
      "0  Prediction  \n",
      "1  Prediction  \n",
      "2  Prediction  \n",
      "3  Prediction  \n",
      "4  Prediction  \n",
      "5  Prediction  \n",
      "6  Prediction  \n",
      "7  Prediction  \n",
      "8  Prediction  \n",
      "9  Prediction  \n",
      "üéØ Unique values in Actual and Predicted Labels:\n",
      "  - Actual: [0. 1.]\n",
      "  - Predicted: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/final/confusion_prediction_only.csv\")\n",
    "\n",
    "print(\"üî¢ Shape:\", df.shape)\n",
    "print(\"üßæ Column names:\", df.columns.tolist())\n",
    "print(\"üìå Sample rows:\")\n",
    "print(df.head(10))\n",
    "print(\"üéØ Unique values in Actual and Predicted Labels:\")\n",
    "print(\"  - Actual:\", df[\"actual_label\"].unique())\n",
    "print(\"  - Predicted:\", df[\"predicted_label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05410f4d-9b6b-49fd-bd05-52ae5eccc6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary saved: confusion_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions (assuming you already have these)\n",
    "# y_valid = ground truth labels\n",
    "# y_pred_thresh = predicted labels (0 or 1)\n",
    "\n",
    "# Define confusion matrix components\n",
    "TP = ((y_valid == 1) & (y_pred_thresh == 1)).sum()\n",
    "TN = ((y_valid == 0) & (y_pred_thresh == 0)).sum()\n",
    "FP = ((y_valid == 0) & (y_pred_thresh == 1)).sum()\n",
    "FN = ((y_valid == 1) & (y_pred_thresh == 0)).sum()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": [\"True Positive\", \"False Positive\", \"False Negative\", \"True Negative\"],\n",
    "    \"Count\": [TP, FP, FN, TN],\n",
    "    \"loan_default_proba\": [pd.NA] * 4,\n",
    "    \"actual_label\": [pd.NA] * 4,\n",
    "    \"predicted_label\": [pd.NA] * 4,\n",
    "    \"Source\": [\"Summary\"] * 4\n",
    "})\n",
    "\n",
    "# Save to CSV to be used in Tableau\n",
    "summary_df.to_csv(\"../data/final/confusion_summary.csv\", index=False)\n",
    "print(\"‚úÖ Summary saved: confusion_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf462ab-d458-45de-ae51-f0e2b112784d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
